import{c as S,a as x,d as k,u as T}from"#entry";class v{baseURL;requestQueue=[];isRefreshing=!1;constructor(e){const t=S();this.baseURL=e||t.public.supabaseUrl}async prepareHeaders(e={}){const t=new Headers(e.headers||{});if(!e.skipAuth){const r=await x().getToken();r&&t.set("Authorization",`Bearer ${r}`)}return t.has("Content-Type")||t.set("Content-Type","application/json"),t}async request(e,t={}){if(this.isRefreshing&&!t.skipAuth)return new Promise((i,l)=>{this.requestQueue.push({resolve:i,reject:l,url:e,options:t})});const s=e.startsWith("http")?e:`${this.baseURL}${e}`,r=await this.prepareHeaders(t),n={...t,headers:r};try{const i=await fetch(s,n);return i.status===401&&!t.skipAuth?this.handle401(e,t):i.status===429?this.handleRateLimit(i,e,t):i}catch(i){if(t.retries&&t.retries>0){const l=t.retryDelay||1e3;return await new Promise(d=>setTimeout(d,l)),this.request(e,{...t,retries:t.retries-1,retryDelay:l*2})}throw i}}async handle401(e,t){const s=x();if(!this.isRefreshing){this.isRefreshing=!0;try{return await s.refreshSession(),this.processQueue(null),this.request(e,t)}catch(r){throw this.processQueue(r),s.logout(),r}finally{this.isRefreshing=!1}}return new Promise((r,n)=>{this.requestQueue.push({resolve:r,reject:n,url:e,options:t})})}async handleRateLimit(e,t,s){const r=e.headers.get("Retry-After"),n=r?parseInt(r)*1e3:5e3;return console.warn(`Rate limited. Retrying after ${n}ms`),await new Promise(i=>setTimeout(i,n)),this.request(t,s)}processQueue(e){this.requestQueue.forEach(({resolve:t,reject:s,url:r,options:n})=>{e?s(e):this.request(r,n).then(t).catch(s)}),this.requestQueue=[]}async get(e,t){return this.request(e,{...t,method:"GET"})}async post(e,t,s){return this.request(e,{...s,method:"POST",body:t?JSON.stringify(t):void 0})}async put(e,t,s){return this.request(e,{...s,method:"PUT",body:t?JSON.stringify(t):void 0})}async delete(e,t){return this.request(e,{...t,method:"DELETE"})}async stream(e,t,s){const r=e.startsWith("http")?e:`${this.baseURL}${e}`,n=await this.prepareHeaders(s);n.set("Accept","text/event-stream");const i=await fetch(r,{...s,headers:n});if(!i.ok||!i.body)throw new Error(`Stream request failed: ${i.statusText}`);const l=i.body.getReader(),d=new TextDecoder;let p="";return(async()=>{try{for(;;){const{done:h,value:f}=await l.read();if(h)break;p+=d.decode(f,{stream:!0});const o=p.split(`
`);p=o.pop()||"";for(const u of o)if(u.startsWith("data: ")){const m=u.slice(6);if(m==="[DONE]")return;try{const c=JSON.parse(m);t(c)}catch(c){console.error("Failed to parse SSE data:",c)}}}}catch(h){throw console.error("Stream reading error:",h),h}})(),()=>{l.cancel()}}}new v;const I={"openai/gpt-oss-120b":{name:"GPT OSS 120B",description:"Most capable open-source model",contextLength:32768,provider:"OpenAI"},"meta-llama/Llama-3.3-70B-Instruct":{name:"Llama 3.3 70B",description:"Latest Llama model",contextLength:128e3,provider:"Meta"},"meta-llama/Meta-Llama-3.1-405B-Instruct-FP8":{name:"Llama 3.1 405B",description:"Largest Llama model",contextLength:128e3,provider:"Meta"},"meta-llama/Meta-Llama-3.1-8B-Instruct":{name:"Llama 3.1 8B",description:"Efficient Llama model",contextLength:128e3,provider:"Meta"},"meta-llama/CodeLlama-13b-Instruct-hf":{name:"CodeLlama 13B",description:"Optimized for coding",contextLength:16384,provider:"Meta"},"mistralai/Mistral-Small-24B-Instruct":{name:"Mistral Small 24B",description:"Compact Mistral model",contextLength:32768,provider:"Mistral"},"mistralai/Mistral-Nemo-Instruct-2407":{name:"Mistral Nemo",description:"Fast and efficient",contextLength:128e3,provider:"Mistral"},"mistralai/Mixtral-8x7B-Instruct-v0.1":{name:"Mixtral 8x7B",description:"MoE architecture",contextLength:32768,provider:"Mistral"},"openGPT-X/Teuken-7B-instruct-commercial":{name:"Teuken 7B",description:"Commercial model",contextLength:8192,provider:"OpenGPT-X"}},C=k("chat",{state:()=>({messages:[],currentModel:"",systemPrompt:"You are a helpful AI assistant supporting teams at HIKEathon 2025. Be concise, accurate, and friendly.",isGenerating:!1,currentStreamingMessage:null,totalTokens:0,contextTokens:0,temperature:.7,maxTokens:2048,topP:.9,abortController:null,availableModels:[],modelsLoading:!1,modelsError:null}),getters:{currentModelInfo:a=>a.availableModels.find(e=>e.id===a.currentModel),contextUsagePercent:a=>{const e=a.availableModels.find(t=>t.id===a.currentModel);return e?Math.min(100,a.contextTokens/e.contextLength*100):0},conversationHistory:a=>a.messages.filter(e=>e.role!=="system")},actions:{async fetchAvailableModels(){if(!this.modelsLoading){this.modelsLoading=!0,this.modelsError=null;try{const{$supabase:a}=T(),e=await fetch(`${a.functions.url}/get-models`,{method:"POST",headers:{"Content-Type":"application/json",Authorization:`Bearer ${a.supabaseKey}`,apikey:a.supabaseKey},body:JSON.stringify({})});if(!e.ok){const s=await e.json();throw new Error(s.error||"Failed to fetch models")}const t=await e.json();if(this.availableModels=t.data?.map(s=>{const r=I[s.id]||{};return{id:s.id,name:r.name||s.id.split("/").pop(),description:r.description||"AI model",contextLength:r.contextLength||8192,provider:r.provider||s.id.split("/")[0]}}).filter(s=>!s.id.includes("bge-")&&!s.id.includes("paraphrase-")&&!s.id.includes("FLUX")&&!s.id.includes("stable-diffusion"))||[],!this.currentModel&&this.availableModels.length>0){const s=this.availableModels.find(r=>r.id==="meta-llama/Meta-Llama-3.1-8B-Instruct");this.currentModel=s?.id||this.availableModels[0].id}}catch(a){console.error("Failed to fetch models:",a),this.modelsError=a.message,this.availableModels=[{id:"meta-llama/Meta-Llama-3.1-8B-Instruct",name:"Llama 3.1 8B",description:"Efficient Llama model",contextLength:128e3,provider:"Meta"}],this.currentModel||(this.currentModel=this.availableModels[0].id)}finally{this.modelsLoading=!1}}},async sendMessage(a,e){if(this.isGenerating)return;const t=e?.map(n=>({id:n.id,name:n.name,type:"image",size:n.size,content:`data:${n.type};base64,${n.base64}`})),s={id:crypto.randomUUID(),role:"user",content:a,timestamp:new Date,attachments:t};this.messages.push(s),this.isGenerating=!0;const r={id:crypto.randomUUID(),role:"assistant",content:"",timestamp:new Date,model:this.currentModel,isStreaming:!0};console.log("Created assistant message:",r.id,"Content:",r.content),this.messages.push(r),this.currentStreamingMessage=r,console.log("Messages array now has:",this.messages.length,"messages");try{const n=[{role:"system",content:this.systemPrompt},...this.messages.slice(-21,-1).map(o=>{const u={role:o.role,content:o.content};if(o.attachments&&o.attachments.length>0){const m=o.attachments.filter(c=>c.type==="image");m.length>0&&(u.images=m.map(c=>({type:"image_url",image_url:{url:c.content}})))}return u})];if(this.abortController=new AbortController,!x().isAuthenticated)throw new Error("Not authenticated. Please login first.");const{$supabase:l}=T(),d=await fetch(`${l.functions.url}/proxy-chat`,{method:"POST",headers:{"Content-Type":"application/json",Authorization:`Bearer ${l.supabaseKey}`,apikey:l.supabaseKey},body:JSON.stringify({model:this.currentModel,messages:n,temperature:this.temperature,max_tokens:this.maxTokens,top_p:this.topP,stream:!0}),signal:this.abortController.signal});if(!d.ok){const o=await d.text();throw console.error("API error response:",o),new Error(`API error: ${d.statusText}`)}const p=d.body?.getReader();if(!p)throw new Error("No response body");const b=new TextDecoder;let h="";for(;;){const{done:o,value:u}=await p.read();if(o)break;h+=b.decode(u,{stream:!0});const m=h.split(`
`);h=m.pop()||"";for(const c of m)if(c.trim()!==""&&c.startsWith("data: ")){const L=c.slice(6).trim();if(L==="[DONE]"){const g=this.messages.findIndex(y=>y.id===r.id);g!==-1&&(console.log("Stream completed, final content:",this.messages[g].content),this.messages[g].isStreaming=!1),this.currentStreamingMessage=null;break}try{const y=JSON.parse(L).choices?.[0]?.delta?.content;if(y){const M=this.messages.findIndex(w=>w.id===r.id);M!==-1&&(this.messages[M].content+=y,console.log("Added content chunk:",y,"Total length:",this.messages[M].content.length))}}catch(g){console.warn("Failed to parse SSE line:",c,g)}}}const f=this.messages.findIndex(o=>o.id===r.id);f!==-1&&(this.messages[f].tokens=this.estimateTokens(this.messages[f].content)),this.updateTokenCounts()}catch(n){if(n.name!=="AbortError"){console.error("Chat error:",n);const i=this.messages.findIndex(l=>l.id===r.id);i!==-1&&(this.messages[i].error=n.message,this.messages[i].content="Sorry, an error occurred while generating the response.",this.messages[i].isStreaming=!1)}this.currentStreamingMessage=null}finally{this.isGenerating=!1,this.abortController=null}},stopGeneration(){this.abortController&&(this.abortController.abort(),this.abortController=null),this.currentStreamingMessage&&(this.currentStreamingMessage.isStreaming=!1,this.currentStreamingMessage=null),this.isGenerating=!1},clearMessages(){this.messages=[],this.totalTokens=0,this.contextTokens=0},deleteMessage(a){const e=this.messages.findIndex(t=>t.id===a);e!==-1&&(this.messages.splice(e,1),this.updateTokenCounts())},editMessage(a,e){const t=this.messages.find(s=>s.id===a);t&&(t.content=e,this.updateTokenCounts())},setModel(a){this.currentModel=a},setSystemPrompt(a){this.systemPrompt=a},setTemperature(a){this.temperature=Math.max(0,Math.min(2,a))},setMaxTokens(a){this.maxTokens=Math.max(1,Math.min(4096,a))},estimateTokens(a){return Math.ceil(a.length/4)},updateTokenCounts(){let a=0,e=this.estimateTokens(this.systemPrompt);for(const t of this.messages){const s=t.tokens||this.estimateTokens(t.content);a+=s,this.messages.indexOf(t)>=this.messages.length-20&&(e+=s)}this.totalTokens=a,this.contextTokens=e}},persist:{paths:["messages","currentModel","systemPrompt","temperature","maxTokens","topP"]}});export{C as u};
